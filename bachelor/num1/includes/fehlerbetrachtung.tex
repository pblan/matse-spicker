\section{Fehlerbetrachtung}

\begin{defi}{Gut gestelltes Problem}
    Praktisch alle Aufgabenstellungen der numerischen Mathematik lassen sich als (eventuell sehr kompliziertes) Nullstellenproblem schreiben:
    \begin{itemize}
        \item gegeben ist eine Funktion $g$ und Eingabedaten $x$
        \item suche $y$ mit $g(x, y) = 0$
    \end{itemize}

    Folgende Eigenschaften sollte ein solches Problem sinnvollerweise erfüllen:
    \begin{itemize}
        \item \emph{Existenz}: es sollte Lösungen geben
        \item \emph{Eindeutigkeit}: zu jedem $x$ sollte es genau ein $y$ geben
        \item \emph{Stetige Abhängigkeit}:\footnote{Stetige Abhängigkeit garantiert, dass (hinreichend) kleine Störungen der Eingabedaten nur kleine Veränderungen der Ergebnisse verursachen.} der Lösung $y$ von den Eingabedaten $x$, d. h.
              \[
                  x' \to x \quad \implies \quad y' = f(x') \to f(x) = y
              \]
    \end{itemize}

    Gilt für das Problem $g(x, y) = 0$ Existenz, Eindeutigkeit und stetige Abhängigkeit, so nennt man das Problem \emph{gut gestellt} (\enquote{well posed}), ansonsten \emph{schlecht gestellt} (\enquote{ill posed}).

    Müssen schlecht gestellte Probleme behandelt werden, so approximiert man sie in der Regel durch gut gestellte Ersatzprobleme (Regularisierung), wobei die Approximationseigenschaften genau kontrolliert werden müssen.
\end{defi}

\begin{example}{Gut gestelltes Problem}

    \begin{itemize}
        \item Ein lineares Gleichungssystem $Ax = b$ ist äquivalent zu:
              \[
                  g(x, y) = 0, \quad x = (A, b), \quad g(x, y) = Ay - b
              \]

              Wir erhalten:
              \[
                  y = f(x) = A^{-1} b, \quad x = (A, b), \quad \det(A) \neq 0
              \]
        \item Die größere der beiden Nullstellen von $t^2 + 2pt - q = 0$, $p, q > 0$ ist gegeben durch:
              \[
                  t_2 = -p + \sqrt{p^2 + q}
              \]
              d. h. das Nullstellenproblem ist äquivalent zu:
              \[
                  g(x, y) = 0, \quad x = (p, q), \quad g(x, y) = y - (-p + \sqrt{p^2 + q})
              \]

              Wir erhalten:
              \[
                  y = f(x) = -p + \sqrt{p^2 + q}, \quad x = (p, q)^T, \quad p, q > 0
              \]
    \end{itemize}

    Beide Funktionen sind stetig, so dass beide Probleme gut gestellt sind.
\end{example}

\subsection{Fehlergrößen}

\begin{defi}{Fehler}
    Sei ein gut gestelltes Problem
    \[
        y = f(x), \quad f: X \to Y
    \]
    gegeben mit:
    \begin{itemize}
        \item $x$ bekannt
        \item $X$ und $Y$ sind Vektorräume mit Normen $\| \cdot \|_X$ bzw. $\| \cdot \|_Y$
    \end{itemize}

    Zur numerischen Behandlung wird $f$ durch ein (diskretes, endliches) Problem $\hat{f}$ ersetzt, das anschließend in Floating-Point-Arithmetik als $\tilde{f}$ implementiert wird.

    Wir kennen also:
    \begin{itemize}
        \item $f(x) = y$:
              \begin{itemize}
                  \item $x$ sind \emph{exakte Eingabedaten}
                  \item $f$ ist ein \emph{gut gestelltes Problem}
                  \item $y$ ist die \emph{exakte Lösung}
              \end{itemize}
        \item $\hat{f}(x) = \hat{y}$:
              \begin{itemize}
                  \item $x$ sind exakte Eingabedaten
                  \item $\hat{f}$ ist das \emph{Ersatzproblem}
                  \item $\hat{y}$ ist die \emph{exakte Lösung des Ersatzproblems}
              \end{itemize}
        \item $\tilde{f}(\tilde{x}) = \tilde{y}$:
              \begin{itemize}
                  \item $\tilde{x}$ sind Eingabedaten in \emph{Floating-Point-Darstellung}
                  \item $\tilde{f}$ ist die \emph{Floating-Point-Implementierung des Ersatzproblems}
                  \item $\tilde{y}$ ist das \emph{Ergebnis des in Floating-Point-Arithmetik durchgeführten Ersatzproblems}
              \end{itemize}
    \end{itemize}

    Die Qualität eines Approximationsverfahrens wird dadurch bestimmt, wie weit $\tilde{y}$ von $y$ abweicht.
    Um diese Abweichung quantitativ zu fassen, werden die folgenden Fehlergrößen definiert:
    \begin{itemize}
        \item \emph{Fehler}:
              \[
                  e := \tilde{y} - y
              \]
        \item \emph{absoluter Fehler}:
              \[
                  e_a := \| \tilde{y} - y \|_Y
              \]
        \item \emph{relativer Fehler}:
              \[
                  e_r := \frac{\| \tilde{y} - y \|_Y}{\| y \|_Y}, \quad \| y \|_Y \neq 0
              \]
    \end{itemize}
\end{defi}

\begin{defi}{Gemischtes Fehlerkriterium}
    In numerischen Verfahren versucht man den Fehler zu kontrollieren und dabei (wenn möglich) unter eine
    vom Benutzer vorgegebene Schranke $\epsilon$ zu drücken.

    Der relative Fehler ist eigentlich aussagekräftiger, kann aber bei kleinen Absolutwerten von $y$ eine unnötig hohe Genauigkeit erzwingen.
    Deshalb benutzt man in der Praxis häufig ein Kriterium, das beide Fehlerarten kombiniert.

    Seien $\epsilon_a$ und $\epsilon_r$ gegebene Schranken für den absoluten bzw. relativen Fehler.

    Beim \emph{gemischten Fehlerkriterium} versucht man eine Approximation $\tilde{y}$ von $y$ zu finden, mit
    \[
        \| \tilde{y} - y \|_Y \leq \epsilon_a + \| y \|_Y \epsilon_r
    \]

    Es gilt:
    \begin{itemize}
        \item Ist der exakte Absolutwert $\| y \|_Y$ sehr klein, dann dominiert $\epsilon_a$ und
              \[
                  e_a = \| \tilde{y} - y \|_Y \lesssim \epsilon_a
              \]
        \item Ist der exakte Absolutwert $\| y \|_Y$ sehr groß, dann dominiert $\|y \|_Y \epsilon_r$ und
              \[
                  \| \tilde{y} - y \|_Y \lesssim \|y \|_Y \epsilon_r \quad \iff \quad e_r = \frac{\| \tilde{y} - y \|_Y}{\| y \|_Y} \lesssim \epsilon_r
              \]
    \end{itemize}
\end{defi}

\subsection{Fehlerquellen}

\subsubsection{Floating-Point-Darstellung von Zahlen}

\begin{defi}{Fließkommazahl}
    Eine \emph{Fließkommazahl} zur Basis $b$ hat die Darstellung
    \[
        v \cdot m \cdot b^e
    \]
    Dabei ist:
    \begin{itemize}
        \item $b \in \N$ die \emph{Basis}
        \item $v = \pm 1$ das \emph{Vorzeichen}
        \item $m$ die \emph{Mantisse}, ein $b$-adischer Bruch
              \[
                  m = m_{-k} \ldots m_{-1}m_0 . m_1 \ldots m_l \quad \iff \quad m = \sum_{i = -k}^{l} m_i \cdot b^{-i}
              \]
              mit Ziffern $m_i \in \{ 0, 1, \ldots, b-1 \}$ und Stellenzahl $k + l + 1$
        \item $e \in \Z$ der \emph{Exponent}
    \end{itemize}

    In dieser Form sind Fließkommadarstellungen einer Zahl nicht eindeutig, denn durch Änderung des Exponenten erhält man z. B.
    \begin{itemize}
        \item $\num{-43210000}$
        \item $-43.21 \cdot 10^6$
        \item $-4.321 \cdot 10^7$
    \end{itemize}

    Die \emph{normalisierte Darstellung}
    \[
        m = \pm (m_0 . m_1 \ldots m_l) \cdot b^e \quad \iff \quad m = \pm \left( \sum_{i = 0}^{l} m_i b^{-i} \right) \cdot b^e, \quad m_0 \neq 0
    \]
    ist für Zahlen $\neq 0$ eindeutig.
\end{defi}

\begin{defi}{Maschinengenauigkeit}
    Jede reelle Zahl $x \in \R$ kann als normalisierte Fließkommazahl zur Basis $b$ mit unendlich langer Mantisse dargestellt werden.

    Durch Runden erzeugt man aus $x$ eine Fließkommazahl $\tilde{x}$ mit Stellenzahl $l + 1$.
    Dazu wird in der Regel dasjenige $\tilde{x}$ ausgewählt, das $x$ am nächsten liegt (\emph{nearest Rundung}.)

    Betrachten wir normierte Fließkommazahlen zur Basis $b$ mit Mantissenlänge $l$, so bezeichnet
    \[
        \epsilon_M := \frac{1}{2} b^{-l}
    \]
    die \emph{Maschinengenauigkeit}.

    Es gilt:
    \begin{itemize}
        \item $\epsilon_M$ ist der maximale relative Fehler, der bei der Umwandlung einer rellen Zahl (bei nearest Rundung) entstehen kann.
        \item $\epsilon_M$ hängt nur von der Basis und der Mantissenlänge, nicht aber vom Exponenten ab.
    \end{itemize}
\end{defi}

\subsubsection{Eingabefehler, Kondition}

\begin{defi}{Eingabefehler}
    Wegen der Umwandlung der Eingabedaten $x$ in Floating-Point-Darstellung $\tilde{x}$ tritt der \emph{Eingabefehler} immer auf und wird deshalb auch als \emph{unvermeidbarer Fehler} bezeichnet:
    \[
        f(\tilde{x}) - f(x)
    \]

    Die Größe dieses Fehleranteils hängt davon ab, wie empfindlich das Originalproblem $f$ auf Störungen in den Eingabedaten reagiert.
    Der Eingabefehler ist im Wesentlichen nicht änderbar. 
\end{defi}

\begin{defi}{Kondition}
    Sei $f: X \to Y$.
    Die kleinsten Konstanten $\varkappa_a$, $\varkappa_r$ mit
    \[
        \| f(x') - f(x) \|_Y \leq \varkappa_a \| x' - x \|_X
    \]
    \[
        \frac{\| f(x') - f(x) \|_Y}{\| f(x) \|_Y} \leq \varkappa_r \frac{\| x' - x \|_X}{\| x \|_X}
    \]
    und
    \[
        \| x' - x \|_X \leq \delta
    \]
    heißen \emph{absolute Kondition} bzw. \emph{relative Kondition} von $f$ in $x$ und hängen von den benutzten Normen, $x$ und $\delta$ ab.

    Die Konditionszahlen beschreiben, wie stark $f$ Störungen in $x$ schlimmstenfalls verstärkt.

    Kleine Konditionszahlen deuten auf ein gut konditioniertes, große auf ein schlecht konditioniertes Problem hin.

    Gut gestellte aber schlecht konditionierte Probleme können sich genauso verhalten wie schlecht gestellte Probleme.

    Werden die Störungen in $x$ durch die Umwandlung in ein Floating-Point-Format verursacht, d.h. $x' =  \tilde{x}$, so gilt:
    \[
        e_r(x) = \frac{\| \tilde{x} - x \|_X}{\| x \|_X} \leq \epsilon_M
    \]
    bzw.
    \[
        e_r(y) = \frac{\| \tilde{y} - y \|_Y}{\| y \|_Y} \leq \varkappa_r \frac{\| \tilde{x} - x \|_X}{\| x \|_X} \leq \varkappa_r \epsilon_M
    \]
    also
    \[
        e_r(y) \quad \text{falls} \quad \varkappa_r \leq \frac{\epsilon_r}{\epsilon_M}
    \]
    d. h. eine sinnvolle Berechnung von $f(x)$ (relativer Fehler unterhalb $\epsilon_r$) ist nur möglich, wenn die Kondition $\varkappa_r$ in Abhängigkeit von der Maschinengenauigkeit $\epsilon_M$ eine gewisse Grenze nicht überschreitet.
\end{defi}

\begin{bonus}{Konditionszahlen herleiten}
    Für stetig differenzierbares $f$ kann man relativ einfach obere Schranken für die Konditionszahlen herleiten.
    Wir betrachten zunächst den einfachen Fall $f: \R \to \R$.

    Nach dem Mittelwertsatz gilt:
    \[
        f(x') = f(x) + f'(\zeta)(x' - x), \quad \zeta \in [ \min(x, x') , \max(x, x') ]
    \]
    und damit:
    \[
        | f(x') - f(x) | = | f'(\zeta) | | x' - x | \leq \max_{\zeta \in [ \min(x, x') , \max(x, x') ]} | f'(\zeta) | | x' - x |
    \]

    Für alle $x'$ mit $| x' - x | \leq \delta$ gilt:
    \[
        [ \min(x, x') , \max(x, x') ] \in [ x - \delta, x + \delta ]
    \]
    und somit:
    \[
        \zeta \in [ \min(x, x') , \max(x, x') ] \quad \implies \quad | \zeta - x | \leq \delta
    \]

    Damit erhalten wir schließlich die Abschätzungen:
    \[
        \varkappa_a \leq \max_{| \zeta - x | \leq \delta} | f'(\zeta) |
    \]
    \[
        \varkappa_r \leq \frac{| x |}{| f(x) |} \max_{| \zeta - x | \leq \delta} | f'(\zeta) |
    \]
\end{bonus}

\begin{defi}{Fehlerfortpflanzungsformeln der differentiellen Fehleranalyse}
    Da die Abschätzung des Maximums oft recht aufwändig ist, ersetzt man sie häufig durch eine einfachere Näherung. 

    Für zweimal stetig differenzierbares $f$ folgt aus der Taylor-Entwicklung von $f$ um $x$
    \[
        f(x') = f(x) + f'(x)(x' - x) + R, \quad \bigo(|x' - x|^2)    
    \]

    Für kleine $|x' - x|$ vernachlässigt man das Restglied $R$ und erhält Näherung (Linearisierung)
    \[ 
        f(x') \approx f(x) + f'(x' - x)
    \]

    Somit ist 
    \[
        |f(x') - f(x)| \approx |f'(x)| |x' - x|    
    \]
    
    und damit
    \[
        \varkappa_a \approx |f'(x)|    
    \]
    
    Analog erhält man für die Verstärkung des relativen Fehlers:
    \[ 
        \varkappa_r \approx \frac{|x|}{|f(x)|} |f'(x)|
    \]

    Diese Näherungen bezeichnet man als \emph{Fehlerfortpflanzungsformeln der differentiellen Fehleranalyse}.

    Für allgemeines $f: \R^n \to \R^m$ mit Normen $\| \cdot \|_X$ bzw. $\| \cdot \|_Y$ als Vektornormen und $\| \cdot \|_{X, Y}$ als zugehörige Matrixnorm geht man analog vor und erhält: 
    \[
        \varkappa_a \approx \|f'(x)\|_{X, Y}, \quad \varkappa_r \approx \frac{\|x\|_X}{\|f(x)\|_Y} \|f'(x)\|_{X, Y}
    \]
\end{defi}

\subsubsection{Verfahrensfehler}

\begin{defi}{Verfahrensfehler}
    Beim \emph{Verfahrensfehler} wird das in exakter Arithmetik gerechnete Ersatzproblem $\hat{f}$ mit dem Originalproblem $f$ für den selben Input $\tilde{x}$ verglichen:
    \[
        \hat{f}(\tilde{x}) - f(\tilde{x})
    \]

    Die Größe dieses Fehleranteils hängt von der Qualität des Ersatzproblems ab.
\end{defi}

\begin{defi}{Diskretisierungsparameter}
    In der Regel hängen Qualität und Aufwand des Ersatzproblems von einem \emph{Diskretisierungsparameter} $h > 0$ ab. 

    Ein vernünftig gewähltes Ersatzproblem geht für $h \to 0$ in das Originalproblem über, d. h. der Verfahrensfehler geht für $h \to 0$ gegen 0 und der Aufwand in der Regel gegen unendlich.

    Das genaue Verhalten des Verfahrensfehlers wird im Einzelfall für jedes Ersatzproblem separat untersucht, insbesondere das asymptotische Verhalten für $h \to 0$ (\emph{Konvergenzuntersuchung}).
\end{defi}

\begin{example}{Diskretisierungsparameter}
    \begin{itemize}
    \item Die LU-Zerlegung zum Lösen linearer Gleichungssysteme liefert in \emph{endlich} vielen Schritten die exakte Lösung, d.h. es kann $\hat{f} = f$ benutzt werden, weshalb der Verfahrensfehler hier 0 ist.
    \item Für das Originalproblem
    \[
        f(x) = e^x = \sum_{k=0}^{\infty} \frac{x^k}{k!}    
    \]
    
    benutzen wir das Ersatzproblem 
    \[ 
        \hat{f}(x) = \sum_{k=0}^{n_h} \frac{x^k}{k!} = 1 + x + \frac{x^2}{2} + \ldots + \frac{x^{n_h}}{n_h!}, \quad n_h = \left\lfloor \frac{1}{h} \right\rfloor
    \]

    Mit $h \to 0$ gilt $n_h \to \infty$, also auch 
    \[
        \hat{f}(x) \to f(x)    
    \]
    \item Für das Originalproblem
    \[
        f(x) = \int_0^1 g(t) \diff t, \quad g \ \text{stetig}
    \]
    
    benutzen wir als Ersatzproblem einmal die linksseitige Rechteckregel
    \[ 
        \hat{f}_1(g) = \sum_{k=0}^{n_h - 1} \frac{1}{n_h} \cdot g \left( \frac{k}{n_h} \right), \quad n_h = \left\lfloor \frac{1}{h} \right\rfloor
    \]

    bzw. die Trapezregel
    \[
        \hat{f}_2(g) = \sum_{k=0}^{n_h - 1} \frac{1}{2n_h} \cdot \left( g \left( \frac{k}{n_h} \right) + g \left( \frac{k+1}{n_h} \right) \right), \quad n_h = \left\lfloor \frac{1}{h} \right\rfloor
    \]

    Mit $h \to 0$ gilt $n_h \to \infty$, also auch 
    \[
        \hat{f}_1(x) \to f(x) \quad \text{und} \quad \hat{f}_2(x) \to f(x)
    \]
    \end{itemize}
\end{example}

\subsubsection{Akkumulierter Rundungsfehler, Floating-Point-Arithmetik}

\begin{defi}{Akkumulierter Rundungsfehler}
    Das Ersatzproblem $\hat{f}$ besteht aus einer endlichen Anzahl von Elementaroperationen (Additionen, Multiplikationen etc.), die bei der Floating-Point-Implementierung $\tilde{f}$ durch ihre Floating-Point-Äquivalente ersetzt werden.

    Jede Floating-Point-Operation erzeugt in der Regel Rundungsfehler, die sich akkumulieren.

    Die Größe dieses Fehleranteils hängt davon ab, wie robust das Ersatzproblem auf eine Floating-Point-Implementierung reagiert:
    \[
        \tilde{f}(\tilde{x}) - \hat{f}(\tilde{x})
    \]

    Zu einem Ersatzproblem kann es mehrere Floating-Point-Implementierung geben, die mathematisch äquivalent (d.h. in exakter Arithmetik identisch) sind, aber unterschiedliche Floating-Point-Ergebnisse liefern.
\end{defi}

\begin{defi}{Lokaler Fehler}
    Der \emph{lokale Fehler} entsteht durch das Runden des Ergebnisses bei der $i$-ten Floating-Point-Operation $\tilde{f}_i$, da bei der Ersatzproblem-Operation $\hat{f}_i$ derselbe Input $\tilde{x}$ benutzt wird:
    \[
        \tilde{f}_i (\tilde{x}_{i-1}) - \hat{f}_i (\tilde{x}_{i-1})
    \]

    Für die erste Operation gilt: 
    \[ 
        \tilde{x}_1 - \hat{x}_1 = \tilde{f}_1 (\tilde{x}) - \hat{f}_1 (\tilde{x})
    \]
\end{defi}

\begin{defi}{Fehlerverstärkung}
    Der Term 
    \[
        \hat{f}_2 (\tilde{x}_1) - \hat{f}_2 (\hat{x}_1)   
    \]
    beschreibt die \emph{Fehlerverstärkung} des Fehlers $\tilde{x}_1 - \hat{x}_1$ aus dem ersten Schritt.

    Alle weiteren Schritte verlaufen analog.

    In jedem Schritt wird also der vorherige Fehler verstärkt und ein neuer lokaler Fehler hinzuaddiert.
    Deswegen bezeichnet man diesen Fehlertyp als \emph{akkumulierten Rundungsfehler}.

    Falls Rundungsfehler das Ergebnis stark verfälschen (d. h. der akkumulierte Fehler sehr groß ist) nennt man eine Implementierung \emph{instabil}, ansonsten ist sie \emph{stabil}.
    Stabile Implementierungen sind also robust gegenüber Rundungsfehlereffekten.

    Ein hoher akkumulierter Rundungsfehler kann durch eine \enquote{günstigere} Floating-Point-Implementierung des Ersatzproblems verändert werden (Summationsreihenfolgen, Vermeidung von Auslöschungen, etc.).
\end{defi}