\section{Approximation}

\subsection{Lineare Ausgleichsprobleme}

\begin{defi}{Ausgleichspolynom mithilfe von Normalgleichungen}
    Seien
    \[
        A = \begin{pmatrix}
            1      & x_1    & \cdots & x_1^{n-1} \\
            \vdots & \vdots &        & \vdots    \\
            1      & x_m    & \cdots & x_m^{n-1}
        \end{pmatrix}
        \in \R^{m \times n}
        , \quad
        b = \begin{pmatrix}
            y_1    \\
            \vdots \\
            y_m
        \end{pmatrix}
        \in \R^{m}
        , \quad
        x = \begin{pmatrix}
            p_1    \\
            \vdots \\
            p_n
        \end{pmatrix}
        \in \R^{n}
    \]
    mit $m \geq n$, $\rang(A) = n$ (also maximal).

    Dann hat für jedes $b \in \R^m$ das \emph{Minimalproblem}
    \[
        \min_{x \in \R^n} \|Ax - b\|_2
    \]
    genau eine Lösung $\hat{x} \in \R^n$.

    Die Lösung $\hat{x}$ löst auch die \emph{Normalgleichungen}
    \[
        A^T A \hat{x} = A^T b
    \]
    wobei $A^T A \in \R^{n \times n}$ regulär ist.\footnote{$A^TA$ ist auch spd, so dass das Cholesky-Verfahren oder iterative Methoden verwendet werden können.}

    Die Lösung $\hat{x}$ liefert Polynomkoeffizienten für das \emph{Ausgleichspolynom}.
\end{defi}

\begin{example}{Ausgleichspolynom mithilfe von Normalgleichungen}
    Gegeben sei die Matrix $A$ und der Vektor $b$ mit:
    \[
        A =
        \begin{pmatrix}
            0 & 1 & 1 \\
            1 & 0 & 1
        \end{pmatrix}
        , \quad
        b =
        \begin{pmatrix}
            b_1 \\
            b_2
        \end{pmatrix}
    \]

    Bestimmen Sie alle Lösungen $x$ mit $\|Ax - b\|_2$ minimal; also alle Ausgleichspolynome.

    \exampleseparator

    Alle Lösungen $x$ mit $\|Ax - b\|_2$ minimal lösen auch $A^TAx = A^Tb$.

    Es gilt damit:
    \[
        A^TA =
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
            1 & 1
        \end{pmatrix}
        \begin{pmatrix}
            0 & 1 & 1 \\
            1 & 0 & 1
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 0 & 1 \\
            0 & 1 & 1 \\
            1 & 1 & 2
        \end{pmatrix}
    \]
    \[
        A^Tb =
        \begin{pmatrix}
            0 & 1 & 1 \\
            1 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            b_1 \\
            b_2
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_2 \\
            b_1 \\
            b_1 + b_2
        \end{pmatrix}
    \]

    Lösen mit Gauß ergibt:
    \[
        \left(
        \begin{array}{ccc|c}
                1 & 0 & 1 & b_2       \\
                0 & 1 & 1 & b_1       \\
                1 & 1 & 2 & b_1 + b_2
            \end{array}
        \right)
        \sim
        \left(
        \begin{array}{ccc|c}
                1 & 0 & 1 & b_2 \\
                0 & 1 & 1 & b_1 \\
                0 & 1 & 1 & b_1
            \end{array}
        \right)
        \sim
        \left(
        \begin{array}{ccc|c}
                1 & 0 & 1 & b_2 \\
                0 & 1 & 1 & b_1 \\
                0 & 0 & 0 & 0
            \end{array}
        \right)
        \implies
        \begin{array}{c}
            x_1 + x_3 = b_2 \\
            x_2 + x_3 = b_1 \\
            x_3 = \alpha \in \R
        \end{array}
    \]

    Wir erhalten insgesamt also die Lösung:
    \[
        x =
        \begin{pmatrix}
            x_1 \\
            x_2 \\
            x_3
        \end{pmatrix}
        =
        \begin{pmatrix}
            b_2 - \alpha \\
            b_1 - \alpha \\
            \alpha
        \end{pmatrix}
        , \quad \alpha \in \R
    \]
\end{example}

\begin{bonus}{Ausgleichsfunktion mit beliebigen Basisfunktionen}
    Wählt man
    \[
        A =
        \begin{pmatrix}
            \varphi_1(x_1) & \cdots & \varphi_n(x_1) \\
            \vdots         &        & \vdots         \\
            \varphi_1(x_m) & \cdots & \varphi_n(x_m)
        \end{pmatrix}
        \in \R^{m \times n}
    \]
    wobei $\varphi_i(x)$ beliebige Basisfunktionen sind, kann man das Approximationsproblem und damit die Ausgleichsfunktion auf diese beliebigen Basisfunktionen erweitern.
\end{bonus}

\subsection{QR-Zerlegung für lineare Ausgleichsprobleme}

\begin{defi}{QR-Zerlegung für lineare Ausgleichsprobleme}
    Seien $A \in \R^{m \times n}$, $b \in \R^{m}$ und $x \in \R^{n}$ wie bisher definiert mit $m \geq n$, $\rang(A) = n$.

    Dann suchen wir analog zur normalen QR-Zerlegung ein
    \[
        Q = Q_m \cdot \ldots \cdot Q_1, \quad Q_i \ \text{orthonormal}
    \]

    z. B. mit Householder-Matrizen, so dass
    \[
        QA = Q_m \cdot \ldots \cdot Q_1 \cdot A =
        \begin{pmatrix}
            *      & \cdots & \cdots & *      \\
            0      & \ddots &        & \vdots \\
            \vdots & \ddots & \ddots & \vdots \\
            0      & \cdots & 0      & *      \\
                   &        &        &        \\
            0      & \cdots & 0      & 0      \\
            \vdots &        & \vdots & \vdots \\
            0      & \cdots & 0      & 0
        \end{pmatrix}
        =:
        \begin{pmatrix}
            R \\
            0
        \end{pmatrix}
    \]
    wobei $R \in \R^{n \times n}$ eine obere Dreiecksmatrix ist, die regulär ist.

    Benutzen wir
    \[
        Qb = \begin{pmatrix}
            c \\
            d
        \end{pmatrix}
    \]
    wobei $c \in \R^n$ und $d \in \R^{m - n}$, so folgt
    \begin{alignat*}{1}
        \min_{x \in \R^n} \|Ax - b\|_2 & = \min_{x \in \R^n} \|QAx - Qb\|_2                                                    \\
                                       & = \min_{x \in \R^n} \| \begin{pmatrix} R \\ 0 \end{pmatrix} x - \begin{pmatrix} c \\ d \end{pmatrix} \|_2 \\
                                       & = \min_{x \in \R^n} \sqrt{\| Rx - c \|^2_2 + \| d \|^2_2}
    \end{alignat*}

    Zu bestimmen ist dann die Lösung von
    \[
        Rx - c = 0 \quad \iff \quad Rx = c
    \]
    durch Rückwärtseinsetzen.
\end{defi}

\subsection{CGLS}

\begin{bonus}{CGLS-Verfahren}
    Das \emph{CGLS-Verfahren} (\emph{Conjugate Gradient Least Squares}) ist definiert durch:

    Das Verfahren funktioniert wie folgt:
    \begin{enumerate}
        \item $x_0$ gegeben, $p_0 = r_0 = b - Ax_0$
        \item Wiederhole für $k \geq 0$:
              \[ x_{k+1} = x_k + \alpha_k p_k \]
              \[ s_{k+1} = s_k - \alpha_k A p_k, \quad \alpha_k = \frac{\langle r_k, r_k \rangle}{\langle A p_k, A p_k \rangle} \]
              \[ r_{k+1} = A^T s_{k+1} \]
              \[ p_{k+1} = r_{k+1} + \beta_k p_k, \quad \beta_k = \frac{\langle r_{k+1}, r_{k+1} \rangle}{\langle r_k, r_k \rangle}\]
    \end{enumerate}

    Hat $A \in \R^{m \times n}$ vollen Rang, d. h. $A^TA \in \R^{n \times n}$ ist regulär, so liefert das CGLS-Verfahren in exakter Arithmetik nach spätestens $n$ Schritten die eindeutige Lösung des Ausgleichsproblems.

    Hat $A$ keinen vollen Rang, so konvergiert CGLS immer noch gegen einen Minimierer.
\end{bonus}

\subsection{Pseudoinverse}

\begin{defi}{Pseudoinverse}
    Sei $A \in \R^{m \times n}$ beliebig, $b \in \R^m$.

    Dann gibt es genau eine Lösung $x \in \R^n$ für das Ausgleichungsproblem.
    Diese hängt linear von $b$, d. h. es existiert eine eindeutige, von $b$ unabhängige Matrix $A^+ \in \R^{n \times m}$ mit
    \[
        x = A^+ b, \quad \forall b \in \R^m
    \]

    $A^+$ heißt dann \emph{Pseudoinverse} von $A$.\footnote{Für reguläres $A$ ist $A^+ = A^{-1}$}

    Die Pseudoinverse $A^+$ ist durch die folgenden vier \emph{Penrose-Axiome} festgelegt:
    \begin{itemize}
        \item $A^+$ ist eine verallgemeinerte Inverse:
              \[
                  AA^+A = A
              \]
        \item $A^+$ verhält sich wie eine schwache Inverse:
              \[
                  A^+AA^+ = A^+
              \]
        \item Die Matrix $AA^+$ ist hermetisch\footnote{Für reelle Matrizen symmetrisch.}:
              \[
                  (AA^+)^T = AA^+
              \]
        \item Die Matrix $A^+A$ ist ebenfalls hermetisch:
              \[
                  (A^+A)^T = A^+A
              \]
    \end{itemize}
\end{defi}

\begin{example}{Pseudoinverse}
    Gegeben sei die Matrix $A$ und der Vektor $b$ mit:
    \[
        A =
        \begin{pmatrix}
            0 & 1 & 1 \\
            1 & 0 & 1
        \end{pmatrix}
        , \quad
        b =
        \begin{pmatrix}
            b_1 \\
            b_2
        \end{pmatrix}
    \]

    Zusätzlich seien die Lösungen für $A^TAx = A^Tb$ gegeben mit:
    \[
        x =
        \begin{pmatrix}
            b_2 - \alpha \\
            b_1 - \alpha \\
            \alpha
        \end{pmatrix}
        , \quad \alpha \in \R
    \]

    Bestimmen Sie die Pseudoinverse $A^+$.

    \exampleseparator

    Für die Pseudoinverse $A^+$ gilt:
    \[
        A^+b \quad \iff \quad \min \|Az - b\|_2 \quad \text{(gegeben)}
    \]
    Dann gilt:
    \[
        \min_{\alpha \in \R} \|z\|_2 = \min_{\alpha \in \R}
        \|
        \begin{pmatrix}
            b_2 - \alpha \\
            b_1 - \alpha \\
            \alpha
        \end{pmatrix}
        \|
        = \min_{\alpha \in \R}
        \|
        \underbrace{\begin{pmatrix}
                b_2 \\
                b_1 \\
                0
            \end{pmatrix}}_{=: c}
        - \alpha
        \underbrace{
            \begin{pmatrix}
                1 \\
                1 \\
                -1
            \end{pmatrix}}_{=: B}
        \|
        = \min_{\alpha \in \R} \| B \alpha - c \|_2
    \]
    \[
        \iff \quad B^TB\alpha = B^Tc \quad \iff \quad
        \begin{pmatrix}
            1 & 1 & -1
        \end{pmatrix}
        \begin{pmatrix}
            1 \\ 1 \\ -1
        \end{pmatrix}
        \alpha
        =
        \begin{pmatrix}
            1 & 1 & -1
        \end{pmatrix}
        \begin{pmatrix}
            b_2 \\ b_1 \\ 0
        \end{pmatrix}
    \]
    \[
        \iff \quad 3\alpha = b_1 + b_2 \quad \iff \quad \alpha = \frac{b_1 + b_2}{3}
    \]

    \begin{alignat*}{1}
        A^+ b = \quad & c - \alpha B \\
        = \quad       &
        \begin{pmatrix}
            b_2 \\
            b_1 \\
            0
        \end{pmatrix}
        -
        \frac{b_1 + b_2}{3}
        \begin{pmatrix}
            1 \\ 1 \\ -1
        \end{pmatrix}   \\
        = \quad       & \frac{1}{3}
        \begin{pmatrix}
            3b_2 - (b_1 + b_2) \\
            3b_1 - (b_1 + b_2) \\
            b_1 + b_2
        \end{pmatrix}   \\
        = \quad       & \frac{1}{3}
        \begin{pmatrix}
            -b_1 + 2b_2 \\
            2b_1 - b_2  \\
            b_1 + b_2
        \end{pmatrix}   \\
        = \quad       &
        \underbrace{
            \frac{1}{3}
            \begin{pmatrix}
                -1 & 2  \\
                2  & -1 \\
                1  & 1
            \end{pmatrix}
        }_{A^+}
        \begin{pmatrix}
            b_1 \\ b_2
        \end{pmatrix}
    \end{alignat*}
\end{example}

\subsection{Singulärwertzerlegung}

\begin{defi}{Singulärwertzerlegung}
    Sei $A \in \R^{m \times n}$ beliebig.

    Dann gibt es orthonormale Matrizen $U \in \R^{m \times m}$, $V \in \R^{n \times n}$, so dass
    \[
        U^TAV = \Sigma
    \]
    mit
    \[
        \Sigma \in \R^{m \times n}, \quad \Sigma = \diagonal(\sigma_1, \ldots, \sigma_p), \quad p = \min(m , n)
    \]
    und
    \[
        \sigma_1 \geq \ldots \geq \sigma_p \geq 0
    \]

    $U^TAV = \Sigma$ bzw. $A = U\Sigma V^T$ heißt \emph{Singulärwertzerlegung} von $A$ mit \emph{Singulärwerten} $\sigma_i$.

    Es gilt:
    \begin{itemize}
        \item $V\Sigma^T U^T$ ist Singulärwertzerlegung von $A^T$ $\implies$ Singulärwerte von $A$ und $A^T$ identisch
        \item hilt $\sigma_1 \geq \ldots \geq \sigma_r > \sigma_{r+1} = \ldots = \sigma_p = 0$ dann ist $\rang(A) = r$
        \item $\sigma_1^2, \ldots, \sigma_2^2$ sind Eigenwerte von $A^TA$ bzw. $AA^T$\footnote{Alle anderen Eigenwerte sind gleich $0$.}
        \item Die Spalten von $U$, $V$ sind Eigenvektoren von $AA^T$ bzw. $A^TA$.
        \item Ist $r = \rang(A)$, $A = U \Sigma V^T$ mit
              \[
                  \Sigma = \diagonal(\sigma_1, \ldots, \sigma_r, 0, \ldots, 0) \in \R^{m \times n}
              \]
              dann ist
              \[
                  A^+ = V \Sigma^T U^T, \quad \Sigma^T = \diagonal(\frac{1}{\sigma_1}, \ldots, \frac{1}{\sigma_r}, 0, \ldots, 0) \in \R^{n \times m}
              \]
        \item Ist $A \in \R^{m \times n}$ und sind $S \in \R^{m \times m}$ und $T \in \R^{n \times n}$ orthonormal, dann haben $A$ und $SAT$ dieselben Singulärwerte.
    \end{itemize}
\end{defi}

\begin{bonus}{Singulärwertzerlegung von spd Matrizen}
    Ist $A \in \R^{n \times n}$ spd, dann gilt
    \[
        A = Q \Lambda Q^T, \quad \Lambda = \diagonal(\lambda_1, \ldots, \lambda_n), \quad Q = \begin{pmatrix} q_1 & \cdots & q_n \end{pmatrix}
    \]
    wobei
    \[
        \lambda_n \geq \ldots \geq \lambda_n
    \]
    die Eigenwerte von $A$ und $q_i$ die zugehörigen Eigenvektoren sind, d. h.
    \[
        \Sigma = \Lambda, \quad U = V = Q
    \]
\end{bonus}

\begin{defi}{Bestimmen einer Singulärwertzerlegung}
    Wir benutzen orthonormale Matrizen $S \in \R^{m \times m}$ und $T \in \R^{n \times n}$ und um $A$ so umzuformen, dass die Singulärwerte leichter zu bestimmen sind.\footnote{Wir nehmen $m \geq n$ an, ansonsten wählen wir $A^T$ statt $A$.}

    \begin{enumerate}
        \item Wir transformieren $A$ durch $S$ und $T$ in eine obere Bidiagonalform:\footnote{Analog zur Hessenberg-Transformation bei der Eigenwertberechnung.}
              \begin{itemize}
                  \item Mit Hilfe von orthogonalen Transformationen (z. B. Householder-Matrizen) $S_i$ eliminieren wir die $i$-te Spalte von $A$:\footnote{Am Beispiel von $S_1$}
                        \[
                            A \to S_1 A =
                            \begin{pmatrix}
                                *      & *      & * & \cdots & *      \\
                                0      & *      & * & \cdots & *      \\
                                0      & *      & * & \cdots & *      \\
                                \vdots & \vdots &   &        & \vdots \\
                                0      & *      & * & \cdots & *      \\
                            \end{pmatrix}
                        \]
                  \item Mit Hilfe von orthogonalen Transformationen (z. B. Householder-Matrizen) $T_i$ eliminieren wir die um eins verkürzte $i$-te Zeile von $A$:\footnote{Am Beispiel von $S_1 A$}
                        \[
                            S_1 A \to S_1 A T_1 =
                            \begin{pmatrix}
                                *      & *      & 0 & \cdots & 0      \\
                                0      & *      & * & \cdots & *      \\
                                0      & *      & * & \cdots & *      \\
                                \vdots & \vdots &   &        & \vdots \\
                                0      & *      & * & \cdots & *      \\
                            \end{pmatrix}
                        \]
                  \item Nach $n$-Schritten erhalten wir
                        \[
                            SAT = S_n \cdot \ldots \cdot S_1 \cdot A \cdot T_1 \cdot \ldots \cdot T_{n-1} =
                            \begin{pmatrix}
                                *      & *      & 0      & \cdots & 0      \\
                                0      & \ddots & \ddots & \ddots & \vdots \\
                                \vdots & \ddots & *      & *      & 0      \\
                                \vdots &        & \ddots & *      & *      \\
                                0      & \cdots & \cdots & 0      & *      \\
                                       &        &        &        &        \\
                                0      & \cdots & \cdots & \cdots & 0      \\
                                \vdots &        &        &        & \vdots \\
                                0      & \cdots & \cdots & \cdots & 0
                            \end{pmatrix}
                            =
                            \begin{pmatrix}
                                R \\ 0
                            \end{pmatrix}
                            , \quad R \in \R^{n \times n}
                        \]
              \end{itemize}
        \item Analogon einer QR-Iteration ohne Shift auf $R^TR$:
              \begin{itemize}
                  \item $R_0 = R$
                  \item Wiederhole für $k > 0$ (immer zwei Schritte $R_k \to R_{k+2}$):
                        \[ R_k^T = Q_{k+1} R_{k+1}, \quad R_{k+1}^T = Q_{k+2} R_{k+2} \]
                  \item $R_{2k}$ konvergiert gegen eine Diagonalmatrix, die die Singulärwerte enthält.
              \end{itemize}
    \end{enumerate}
\end{defi}

\begin{example}{Bestimmen einer Singulärwertzerlegung}
    Gegeben sei die Matrix $A$  mit:
    \[
        A =
        \begin{pmatrix}
            0 & 1 & 1 \\
            1 & 0 & 1
        \end{pmatrix}
    \]

    Bestimmen Sie die Singulärwerte von $A$.

    \exampleseparator

    Die Singulärwerte $\sigma_i$ von $A$ lassen sich über die Eigenwerte $\lambda_i$ von $A^TA$ bestimmen, da
    \[
        \sigma_i^2 = \lambda_i
    \]

    Es gilt:
    \[
        A^TA =
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
            1 & 1
        \end{pmatrix}
        \begin{pmatrix}
            0 & 1 & 1 \\
            1 & 0 & 1
        \end{pmatrix}
        =
        \begin{pmatrix}
            1 & 0 & 1 \\
            0 & 1 & 1 \\
            1 & 1 & 2
        \end{pmatrix}
    \]

    \[
        p(\lambda) = \det(A^TA - \lambda I) = \det
        \begin{pmatrix}
            1 - \lambda & 0           & 1           \\
            0           & 1 - \lambda & 1           \\
            1           & 1           & 2 - \lambda
        \end{pmatrix}
        = \ldots = (1-\lambda)(\lambda-3)\lambda
    \]
    \[
        0 = p(\lambda) = (1-\lambda)(\lambda-3)\lambda \quad \implies \quad \lambda_1 = 3, \lambda_2 = 1, \lambda_3 = 0
    \]

    Damit sind die Singulärwerte von $A$ direkt:
    \[
        \sigma_1 = \sqrt{3}, \quad \sigma_2 = 1
    \]


\end{example}

\subsection{Regularisierung schlecht konditionierter Probleme}

\subsubsection{Grundlagen}

\begin{defi}{Konditionszahl mithilfe von Singulärwerten}
    Es sei $A \in \R^{m \times n}$ mit Singulärwertzerlegung $A = U \Sigma V^T$
    \[
        \Sigma = \diagonal(\sigma_1, \ldots, \sigma_r, 0, \ldots, 0) \in \R^{m \times n}, \quad \sigma_1 \geq \ldots \geq \sigma_r > 0
    \]

    Dann ist die Konditionszahl $\varkappa_2$ von $A$ gegeben durch\footnote{Für reguläre Matrizen $A$ stimmt dieser Ausdruck mit der früheren Definiton der Konditionszahl $\varkappa_2$ überein.}
    \[
        \varkappa_2 (A) = \frac{\sigma_1}{\sigma_r}
    \]
\end{defi}

\subsubsection{Abgeschnittene Singulärwertzerlegung (TSVD)}

\begin{bonus}{Abgeschnittene Singulärwertzerlegung}
    Konditionsprobleme werden durch die Singulärwerte $\sigma_i$ von $A$ verursacht, für die
    \[
        \frac{\sigma_1}{\sigma_i}
    \]
    groß ist.

    Lassen wir diese $\sigma_i$ einfach weg, dann erhalten wir die \emph{abgeschnittene Singulärwertzerlegung (Truncated Singular Value Decomposition, TSVD)}.

    Es sei $A \in \R^{m \times n}$ mit Singulärwertzerlegung $A = U \Sigma V^T$,
    \[
        \Sigma = \diagonal(\sigma_1, \ldots, \sigma_r, 0, \ldots, 0) \in \R^{m \times n}, \quad \sigma_1 \geq \ldots \geq \sigma_r > 0
    \]

    Für $\alpha > 0$ ist $A_\alpha = U \Sigma_\alpha V^T$,
    \[
        \Sigma_\alpha = \diagonal(\sigma_1, \ldots, \sigma_k, 0, \ldots, 0) \in \R^{m \times n}, \quad \frac{\sigma_1}{\sigma_k} \leq \frac{1}{\alpha}, \quad \frac{\sigma_1}{\sigma_{k+1}} \geq \frac{1}{\alpha}
    \]
    eine \emph{abgeschnittene Singulärwertzerlegung} von $A$.
\end{bonus}

\subsubsection{Tikhonov Regularisierung}

\begin{defi}{Tikhonov Regularisierung}
    Für $A \in \R^{m \times n}$ und $\alpha > 0$ ist
    \[
        x_\alpha = (A^TA + \alpha^2 I)^{-1} A^T b
    \]
    die \emph{Tikhonov-Regularisierung} von $x = A^+ b$ zum Parameter $\alpha$.
\end{defi}

\begin{bonus}{Konvergenz der Tikhonov Regularisierung}
    Sei $x_\alpha$ die Tikhonov-Regularisierung von $x = A^+ b$ zum Parameter $\alpha$.

    Für $\alpha \to 0$ konvergiert $x_\alpha$ gegen $x$ und das Residuum $\|b - A x_\alpha\|_2$ ist monoton nicht wachsend.
\end{bonus}

\subsubsection{Parameterwahl}

\begin{bonus}{Diskrepanz-Prinzip nach Morozov}
    Seien $A$, $\tilde{b}$, $\delta$ gegeben, $\|\tilde{b} - b\|_2 \leq \delta$ und $\tilde{x}_\alpha$ eine Regularisierung von $A^+ \tilde{b}$.

    Dann wählt man den Regularisierungsparameter gemäß
    \[
        \alpha(\delta, \tilde{b}) = \sup_{\alpha > 0} \left( \|\tilde{b} - A \tilde{x}_\alpha\|_2 \leq \tau \delta \right)
    \]
    mit $\tau \geq 1$ fest.
\end{bonus}
